JCrawler
----------------------------
Yet another super basic web crawler. For course: Web Search Engines

Work flow:

1. accept input key words and number of result pages intended to be crawled

2. ask Google for top 10 websites for these key words as seed sites

3. do focused crawling according to predicted priority (how likely key words would appear in a url), dealing with some cases(Robots.txt, relative URL, etc.) along the way, print beautiful logs into terminal as well as LOGFILE

4. parse page data, give estimated priority to urls and add them to url priority queue; count actual score of current page at the same time

5. write pages and their metadata into file after fully processed, to minimize disk IO

Feature:

1. using configurable multiple threads for downloading, maximize bandwith utilization

2. adjust parsing thread speed when number of fetched URLs "overshoots" desired number of sites


Todo:

1. downloading is bottleneck: try implementing pre-DNS resolvation feature to help

2. try non-blocking paradigm